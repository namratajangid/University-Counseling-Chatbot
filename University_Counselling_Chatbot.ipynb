{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "University Counselling Chatbot.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/namratajangid/University-Couseling-Chatbot/blob/master/University_Counselling_Chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CNW0m22UcRVs",
        "colab_type": "text"
      },
      "source": [
        "## **Importing Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "leXNlTAtcNat",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# importing chat-bot intents file\n",
        "import json\n",
        "with open('intents.json') as json_data:\n",
        "    intents = json.load(json_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IxtztHsBdbUp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# importing libraries required for NLP\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.stem.lancaster import LancasterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "stemmer = LancasterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "from tensorflow import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Dropout\n",
        "from keras import utils\n",
        "from keras import layers\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IxWU6hZBbtZr",
        "colab_type": "text"
      },
      "source": [
        "## **Processing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y3nsiO1ydwM5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "words = []\n",
        "classes = []\n",
        "documents = []\n",
        "ignore_words = ['?']\n",
        "# loop through each sentence in our intents patterns\n",
        "for intent in intents['intents']:\n",
        "    for pattern in intent['patterns']:\n",
        "        # tokenize each word in the sentence\n",
        "        w = nltk.word_tokenize(pattern)\n",
        "        # add to our words list\n",
        "        words.extend(w)\n",
        "        # add to documents in our corpus\n",
        "        documents.append((w, intent['tag']))\n",
        "        # add to our classes list\n",
        "        if intent['tag'] not in classes:\n",
        "            classes.append(intent['tag'])\n",
        "\n",
        "# stem and lower each word and remove duplicates\n",
        "words = [lemmatizer.lemmatize(w.lower()) for w in words if w not in ignore_words]\n",
        "words = sorted(list(set(words)))\n",
        "\n",
        "# remove duplicates\n",
        "classes = sorted(list(set(classes)))\n",
        "\n",
        "print (len(documents), \"documents\", documents)\n",
        "print (len(classes), \"classes\", classes)\n",
        "print (len(words), \"unique stemmed words\", words)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_3EOTvdmd2EJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create our training data\n",
        "training = []\n",
        "output = []\n",
        "# create an empty array for our output\n",
        "output_empty = [0] * len(classes)\n",
        "\n",
        "# training set, bag of words for each sentence\n",
        "for doc in documents:\n",
        "    # initialize our bag of words\n",
        "    bag = []\n",
        "    # list of tokenized words for the pattern\n",
        "    pattern_words = doc[0]\n",
        "    # stem each word\n",
        "    pattern_words = [lemmatizer.lemmatize(word.lower()) for word in pattern_words]\n",
        "    # create our bag of words array\n",
        "    for w in words:\n",
        "        bag.append(1) if w in pattern_words else bag.append(0)\n",
        "\n",
        "    # output is a '0' for each tag and '1' for current tag\n",
        "    output_row = list(output_empty)\n",
        "    #print(classes.index(doc[1]))\n",
        "    output_row[classes.index(doc[1])] = 1\n",
        "\n",
        "    training.append([bag, output_row])\n",
        "    #print(pattern_words)p"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YlZ6rQ6Pd59E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# shuffle our features and turn into np.array\n",
        "random.shuffle(training)\n",
        "training = np.array(training)\n",
        "\n",
        "# create train and test lists\n",
        "train_x = list(training[:,0])  #prints the first column of the matrix (patterns)\n",
        "train_y = list(training[:,1])  #prints the second column of the matrix (tags/context)\n",
        "print(train_x)\n",
        "print(train_y) \n",
        "print(len(train_x))\n",
        "print(len(train_y))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l-rND5freA4A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Dense(40, input_shape=[len(train_x[0])]))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(40))\n",
        "#model.add(Dense(40))\n",
        "model.add(Dense(len(train_y[0]), activation='softmax'))\n",
        "\n",
        "#model.summary()\n",
        "#model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])\n",
        "#history = model.fit(np.array(train_x), np.array(train_y), epochs=500, batch_size=16)\n",
        "#print(len(model.weights)) 6\n",
        "\n",
        "\n",
        "#Taken from a pervious version \n",
        "#model = Sequential()\n",
        "#model.add(Dense(40, input_shape=[len(train_x[0],)]))\n",
        "#model.add(Dropout(0.20))\n",
        "#model.add(Dense(40))\n",
        "#model.add(Dense(len(train_y[0]), activation='softmax'))\n",
        "\n",
        "model.summary()\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])\n",
        "es = keras.callbacks.EarlyStopping(monitor='loss', mode='min', verbose=1, baseline = '0.01', patience=20)\n",
        "history = model.fit(np.array(train_x), np.array(train_y), epochs=100, batch_size=16, callbacks = es)\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1OTtLqUKeFhA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.save('model_ChatBot.h5')\n",
        "history_dict = history.history\n",
        "history_dict.keys()\n",
        "acc = history.history['acc']\n",
        "loss = history.history['loss']\n",
        "epochs = range(1, len(acc) + 1)\n",
        "plt.plot(epochs, acc,'bo', label='Training acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.legend()\n",
        "plt.figure()\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cJrG1I-oZq0D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Dense(40, input_shape=[len(train_x[0],)]))\n",
        "model.add(Dense(40))\n",
        "#model.add(Dense(40))\n",
        "model.add(Dense(len(train_y[0]), activation='softmax'))\n",
        "\n",
        "\n",
        "model.summary()\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])\n",
        "history = model.fit(np.array(train_x), np.array(train_y), epochs=100, batch_size=16)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V5h16U11YSee",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.save('model_ChatBot.h5')\n",
        "history_dict = history.history\n",
        "history_dict.keys()\n",
        "acc = history.history['acc']\n",
        "loss = history.history['loss']\n",
        "epochs = range(1, len(acc) + 1)\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.plot(epochs, acc,'bo', label='Training acc')\n",
        "plt.xlabel('Number of Epochs',fontsize=30)\n",
        "plt.ylabel('Accuracy',fontsize=30)\n",
        "plt.xticks(fontsize=30)\n",
        "plt.yticks(fontsize=30)\n",
        "\n",
        "plt.title('Training and validation accuracy',fontsize=30)\n",
        "plt.legend(loc=\"lower right\",fontsize=20)\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "plt.title('Training and validation loss',fontsize=20)\n",
        "\n",
        "plt.legend(fontsize=20)\n",
        "plt.xlabel('Number of Epochs',fontsize=20)\n",
        "plt.ylabel('Loss',fontsize=20)\n",
        "plt.xticks(fontsize=20)\n",
        "plt.yticks(fontsize=20)\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wwp46v0eeGPe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def clean_up_sentence(sentence):\n",
        "    # tokenize the pattern\n",
        "    sentence_words = nltk.word_tokenize(sentence)\n",
        "    # stem each word\n",
        "    sentence_words = [lemmatizer.lemmatize(word.lower()) for word in sentence_words]\n",
        "    return sentence_words\n",
        "\n",
        "# return bag of words array: 0 or 1 for each word in the bag that exists in the sentence\n",
        "def bow(sentence, words, show_details=True):\n",
        "    # tokenize the pattern\n",
        "    sentence_words = clean_up_sentence(sentence)\n",
        "    # bag of words\n",
        "    bag = [0]*len(words) \n",
        "    for s in sentence_words:\n",
        "        for i,w in enumerate(words):\n",
        "            if w == s: \n",
        "                bag[i] = 1\n",
        "                if show_details:\n",
        "                    print (\"found in bag:\", w)\n",
        "\n",
        "    return(np.array(bag))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ho52ocbZeQgh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create a data structure to hold user context\n",
        "context = {}\n",
        "\n",
        "ERROR_THRESHOLD = 0.55\n",
        "def classify(sentence):\n",
        "    # generate probabilities from the model\n",
        "    p = bow(sentence, words)\n",
        "    d = len(p)\n",
        "    f = len(documents)-2\n",
        "    a = np.zeros([f, d])\n",
        "    tot = np.vstack((p,a))\n",
        "    \n",
        "    results = model.predict(tot)[0]\n",
        "    #print(\"Model preditions: \", len(results))\n",
        "    #for i in results:\n",
        "     # print(i*100)\n",
        "    \n",
        "    # filter out predictions below a threshold\n",
        "    results = [[i,r] for i,r in enumerate(results) if r>ERROR_THRESHOLD]\n",
        "    # sort by strength of probability\n",
        "    results.sort(key=lambda x: x[1], reverse=True)\n",
        "    return_list = []\n",
        "    for r in results:\n",
        "        return_list.append((classes[r[0]], r[1]))\n",
        "    # return tuple of intent and probability\n",
        "    #print(\"Return list \", len(return_list))\n",
        "    #print(return_list)\n",
        "    return return_list\n",
        "\n",
        "def response(sentence, userID, actual_tag_list, show_details=False):\n",
        "    results = classify(sentence)\n",
        "    result = results[0]\n",
        "    print('Resulting Intent:',result[0])\n",
        "    print('Probability:',result[1])\n",
        "    # if we have a classification then find the matching intent tag\n",
        "    if results:\n",
        "        # loop as long as there are matches to process\n",
        "        while results:\n",
        "            for i in intents['intents']:\n",
        "                # find a tag matching the first result\n",
        "                if i['tag'] == results[0][0]:\n",
        "                    # set context for this intent if necessary\n",
        "                    if 'context_set' in i:\n",
        "                        if show_details: print ('context:', i['context_set'])\n",
        "                        context[userID] = i['context_set']\n",
        "\n",
        "                    # check if this intent is contextual and applies to this user's conversation\n",
        "                    if not 'context_filter' in i or \\\n",
        "                        (userID in context and 'context_filter' in i and i['context_filter'] == context[userID]):\n",
        "                        if show_details: \n",
        "                          print ('Intent Tag:', i['tag'])\n",
        "                          if (i['tag']!=['']):\n",
        "                            actual_tag_list.append(i['tag'])\n",
        "                          if (i['tag']==['']):\n",
        "                            actual_tag_list.append(\"emptytag\")\n",
        "                        # a random response from the intent\n",
        "                        print(random.choice(i['responses']))\n",
        "                        return (random.choice(i['responses']), actual_tag_list)\n",
        "            results.pop(0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B3IcsYg0t2BD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Testing\n",
        "query_list = [\"How can I get admission in B.Tech?\", \"Highest package of btech\", \"Hey can you tell me about the college campus\", \"Compare between cs and it\", \"Why computer and why not it\", \"Btech integrated admissions\", \"Tell me about yourself\", \"btech integrated details\", \"how much does a course costs\", \"should i go for engineering in civil\", \"give me placement results\", \"can you provide contact information\",\"Programs offered?\",\"Courses offered?\",\"PG programs?\",\"MBA Tech placements?\",\"International Companies on campus?\",\"Hostel Facilities?\",\"Can I get information on fee structure for B.Tech course?\",\"MPSTME campus?\",\"Data Science\",\"Compare B.Tech and MBA Tech\",\"Why should I opt for MCA?\",\"Can you assist me with M.Tech admission process?\",\"Take me to college website\",\"Courses offered at Shirpur Campus?\",\"Library Facilities\",\"List of phd courses\",\"List of M.Tech courses\",\"Programs\",\"Phd\",\"PhD mentor list\",\"Course Structure\",\"Fees for PhD\",\"Discipline for PhD\",\"How to apply?\",\"How to apply for PhD admission?\",\"Whats about placement?\",\"International Firms\",\"Do you have any scholarship?\",\"Stipend in masters program?\",\"Postgraduate program\",\"Seats for PhD\",\"Intake for post graduation\",\"Seats in B.Tech\",\"Intake for B.Tech\",\"Intake for MCA\",\"Entrance procedure for B.Tech\",\"Admission in B.Tech\",\"MBA Tech. Admissions\"]\n",
        "\n",
        "expected_tag_list = [\"btechandmbatechadmissions\", \"placements\", \"campus\", \"itVsCS\", \"itVsCS\", \"otherprogramsadmissions\", \"aboutcollege\", \"CourseB.TechInt\", \"generalfeestructure\", \"whyCivil\", \"placements\", \"contactDetails\", \"generalPrograms\", \"generalCourses\", \"postgradprograms\", \"placements\", \"placements\", \"hostelInfo\", \"generalfeestructure\", \"campus\", \"streamDataScience\", \"btechVSmbatech\", \"whyMCA\", \"otherprogramsadmissions\", \"aboutcollege\", \"generalCourse\", \"campus\", \"CoursePhD\", \"CourseM.Tech\", \"ambiguousQueries\", \"ambiguousQueries\", \"CoursePhD\", \"courseStructure\", \"generalfeestructure\", \"CoursePhD\", \"otherprogramsadmissions\", \"otherprogramsadmissions\", \"placements\", \"placements\", \"scholarship\", \"CoursePhD\", \"postgradprograms\", \"CoursePhD\", \"postgradprograms\", \"CourseB.Tech\", \"CourseB.Tech\", \"CourseMCA\", \"btechandmbatechadmissions\", \"btechandmbatechadmissions\", \"btechandmbatechadmissions\"]\n",
        "actual_tag_list = []\n",
        "print(len(query_list))\n",
        "for i in query_list:\n",
        "  ans = response(i, \"123\", actual_tag_list, show_details = True)\n",
        "  #print(ans[0])\n",
        "  \n",
        "print(len(query_list))\n",
        "print(len(expected_tag_list))\n",
        "print(len(ans[1]))\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "passed = 32\n",
        "failed = 18\n",
        "  \n",
        "\n",
        "print(\"Test cases passed: \", passed)\n",
        "print(\"Test cases failed: \", failed)\n",
        "      \n",
        "PassPercent = (passed*100)/50\n",
        "print(PassPercent)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E3eXk-eJGIhk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "outputId": "555379d0-d6e5-4abf-9fca-e5dea8195ef3"
      },
      "source": [
        "response(\"how can i reach college?\", \"123\",actual_tag_list, show_details = True)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "found in bag: how\n",
            "found in bag: can\n",
            "found in bag: i\n",
            "found in bag: reach\n",
            "found in bag: college\n",
            "Resulting Intent: contactDetails\n",
            "Probability: 0.9984549\n",
            "Intent Tag: contactDetails\n",
            "You can reach us at: Bhakti Vedant Swami Marg, Near Cooper Hospital, JVPD Scheme, Vile Parle (West), Mumbai. Maharashtra- 400 056, India. Tel. No: (+91 22 4233 4000) Email: (enquiry.mpstme@nmims.edu) / (admissions.MPSTME@nmims.edu)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('You can reach us at: Bhakti Vedant Swami Marg, Near Cooper Hospital, JVPD Scheme, Vile Parle (West), Mumbai. Maharashtra- 400 056, India. Tel. No: (+91 22 4233 4000) Email: (enquiry.mpstme@nmims.edu) / (admissions.MPSTME@nmims.edu)',\n",
              " ['btechandmbatechadmissions', 'placements', 'contactDetails'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2F3A0oNWvUKN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        },
        "outputId": "0d27da6f-b1e7-4f21-c61d-10e0be54ddd7"
      },
      "source": [
        "response(\"which is better CS or IT\", \"123\",actual_tag_list, show_details = True)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "found in bag: which\n",
            "found in bag: is\n",
            "found in bag: c\n",
            "found in bag: it\n",
            "Resulting Intent: streamComputer\n",
            "Probability: 0.82703406\n",
            "Intent Tag: streamComputer\n",
            "We play a significant role in creating Computer Engineering Graduates with sound technical and managerial skills of value to industry and society both at national and international level.We provide Computer Engineering Stream in the following courses: B. Tech, B. Tech Integrated, MBA Tech, M. Tech\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('We play a significant role in creating Computer Engineering Graduates with sound technical and managerial skills of value to industry and society both at national and international level.We provide Computer Engineering Stream in the following courses: B. Tech, B. Tech Integrated, MBA Tech, M. Tech',\n",
              " ['btechandmbatechadmissions',\n",
              "  'placements',\n",
              "  'contactDetails',\n",
              "  'dean',\n",
              "  'streamComputer'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    }
  ]
}